逸仙时空BBS搜索引擎设计与实现

摘要
自文字产生以来，如何快速地从大量的，记录在各种各样的存储媒体中的信息中找到找到所需信息就成为一个引人注目的问题。
逸仙时空BBS作为校内的信息交流平台，同学们的网上家园，但系统却没有一个支持全文检索的功能，本着整合信息资源，方便用户查找信息的目的出发，本文设计实现了一个针对逸仙时空BBS的小型搜索引擎。
本文介绍了搜索引擎的背景和发展过程，讨论了搜索引擎的工作原理和技术，介绍了BBS相关的内容和内部的个工作机制，提出了BBS对搜索引擎的特殊要求。
本文还阐述了设计该搜索引擎的理念和原则，我们致力打造一个实用的，可扩展性强，易维护的轻量级搜索引擎。 此外，本文还描述了一些实现细
节。最后对系统进行运行测试，并进行了效果分析和性能分析，提出了改进方向和目标。

关键词: 公告版系统，搜索引擎，信息检索，


Abstarct

Since the text has been invernted, how to find the information needed from a large a number of information recoreded in a wide range of storage media has become a  high-profile issues. Yat-sen Channel BBS is an information exchange platform and the online home for students of Sun Yat-sen University. But but the system dos not support the full-text search of articles. On the aim of integrate the information resources to facilitate users to find information, we designed a small search engine for the Yat-sen BBS in this article.
      This article introduces the background and development process of search engine, discusses the work of search engine
principles and core technology. Also this article described the BBS-related content and the internal working mechanism,  discusses the special requirements of search engines for the BBS. This article also described the design concept of the search engine and
principles, we are committed to creating a usable, scalable, and lightweight easy to maintain search engine. In addition, the paper also describes some implementation details. Finally, we run the system test, do analysis of the results and do the performance analysis for future improvment.

keywords: BBS, search engine, information retrieval


1. 前言
1.1 搜索引擎介绍
自文字产生以来，如何快速地从大量的，记录在各种各样的存储媒体中的信息就
成为一个引人注目的问题。这个问题关系到人类如何能够主动地获取自己需要的知识。
因此，文本信息检索技术甚至可以追述到古代的书籍编目。但是直到近一个世纪，随着
人类的知识以前所未有的速度急剧膨胀，信息存储方式越来越丰富，使得在海量的，多
模态的信息库中进行快速、准确的检索成为急迫的需求。1945年，Vannevar Bush的论文
《As We May Think》[1]第一次提出了设计自动的，在大规模的存储数据中进行查
找的机器的构想，这被认为是现在信息检索技术的开山之作。

1.1.1  搜索引擎历史
1990年以前，没有任何人能搜万维网。所有搜索引擎的祖先是1990年由Montreal的McGill
Uniserversity学生AlanEmtage，Peter Deutsch和Bill Wheelan发明的Archie[2]。
它是第1个自动索引万维网匿名FTP网站文件的程序，但还不是真正的网络搜索引擎(Web Search Engine)。
世界上第一个网路爬虫(Spider)[3]程序是由麻省理工学院(MIT)的学生Matthew Gray编写的
的World Wide Web Wanderer，用于追踪万维怕网的发展规模。
NASA的Repository-BasedSoftware Engineering(RBSE)[4]是第一个索引HTML文件正文的搜
索引擎，也是第1个在搜索结果排列中引入关键字匹配程度概念的引擎。
1998年Stanford University的Larry Page及Sergey Brin提出了PageRank概念[5]，并把它运用在一个新型搜索引擎的研究项目中，后来他们创建了Google搜索引擎。
在英语世界中，"Google"已经成为“查询，搜索”的代名词，而中文中也出现了“知之为知之，不知Google之”，“内事不决问老婆，外事不决问Google”等类似的新说法。

1.1.2 搜索引擎的需求
万维网的高速发展迫切地要求一种快速，全面，准确，稳定可靠的信息查询方法。由于搜索
引擎满足了这4个需求，所以才奠定了其不可替代的地位。

1.1.3 搜索引擎的体系结构:
一般的搜索引擎结构清晰，分工明确。 按照各自的功能划分，分为以下4大系统:
1. 下载系统. 负责从万维网下载各种类型的网页，并且保持对网页变化的同步.
2. 分析系统. 负责抽取下载系统得到的网页数据，并进行PageRank和分词计算
3. 索引系统. 负责将分析系统处理后的网页对象索引入库
4. 查询(检索)系统. 负责分析用户提交的查询请求，然后从索引库中检索出相关网页并
将网页排序后，以查询结果的形式返回给用户.

@搜索引擎简要结构图
architecture.jpg

1.2 BBS介绍
1.2.1 BBS历史与简介
BBS是英文Bullitin Board System的首字母简写，意译即为公告牌系统。一般地它以纯文字的方式出现，工作界面也像最古老的终端，就是一行行的字。这是在计算机网络发展早期甚至在单主机时代就发展起来的一种网络服务，那时候有那么一群大学生，也可能是科研机构研究员什么的，觉得在Unix主机上面要是能做一个论坛样的东西多好，于是他们就写了一个命令行程序，运行这个程序，操作者可以在界面下面留言，为了让多个人同时可以操作这个系统，他们把这个程序设置为系统某个用户的shell，每个telnet上该主机的用户，只要使用这个用户的用户名和密码登陆，就可以进行交流，这就是Internet BBS的雏形。经过一段时间的发展，这个系统具有了相当多的交互功能，用户不仅可以留言，还可以互相发送信件，发送信息，看到同时在线的用户等等。 BBS 系统开发者们为了让更多的人能使用这个系统并完善之，将BBS以开源协议发布于网络上面。只要拥有Unix主机，就可以取得源代码并安装BBS。因此BBS以很快的速度发展起来。在众多BBS中，某个叫做Pirate BBS ，经过某些人修改后叫做Eagle BBS的分枝，流传入了台湾地区，交大资讯工程系从他发展出了 Phoenix BBS，Phoenix BBS 是如今大部分中文Telnet BBS的祖先，然而它的名字却远不如其后辈响亮，在它的基础上由中正资工进一步修改的 BBS，被赋予了那个大陆 BBS 开发者耳熟能详的名字--Firebird BBS[7]。逸仙时空BBS就是在Firbird2000的基础上发展而来的。

1.2.2 BBS工作原理和特点
对于设计针对BBS的搜索引擎，我们需要进一步了解BBS工作原理和特点。
BBS的界面采用纯文本方式表现，用户使用终端软件连接BBS，文本界面在服务器端生成并发送出来，客户端软件仅原样显示文本内容。
BBS是有不同的版面组成的，版也就是讨论区，每个版有不同的讨论主题。逸仙时空大约有200个讨论区，每个版面文章数目大约在几千到几万篇不等，大小为30-50M。
版面文章更新较快，一个版面在一天内新发表文章可达上千，版主整理版面时，一些文章会被删除。
BBS用文件来存储文章和索引，每一篇文章对应一个文本文件，文件名的格式为 "M.time.A"，其中time为为文章发表时的unix时间戳，也即是time(3)调用返回的结果，例如"M.1234567890.A"，每个版面对应一个文件夹，里面除了保存有所有版面文章外，还有一个文章目录索引文件，文件名为.DIR，.DIR文件记录着每篇文章的元信息:
包括"文件名，作者，发表时间，标题，主题ID，文章标记，文章大小"等。
我们来看一下BBS代码中关于文章头信息的结构体定义
struct fileheader {
       char filename[FNAMELEN];		// filename format: {M|G}.time.{Alphabet}
       char owner[IDLEN + 2];
       char realowner[IDLEN + 2];	// to perserve real owner id even in anonymous board
       char title[TITLELEN];
       unsigned int flag;		// article flag
       unsigned int size; // file size
       unsigned int id;   // identity of article (per thread)
       time_t filetime; // post time
       char reserved[12]; // not used
};
.DIR目录文件就是按时间顺序保存着每篇文章的fileheader结构体，用BNF表示为:
.DIR ::= fileheaders ;
fileheaders ::= fileheaders fileheader | ;
fileheader ::= struct fileheader ;

#精华区
BBS中除了版面文章以外，每个版面都对应着一个精华区，顾名思义，精华区里
保存着本版的精华文章，这是由每个版面的版主从版面文章中整理收录进来的。
精华区的结构就像文件系统里的目录结构，下文还会具体描述。精华区文章数目相对较多，但是精华区文章更新就不那么频繁。

还有值得一提的就是，近年来各大BBS基本都提供了通过web浏览器访问的方式，也就是根据文章内容生成HTML页面显示出来，逸仙时空也不例外。
以上所述的BBS的特点，是这个搜索引擎的各个环节设计需要考虑的地方，具体见后面章节。

2. 系统总体架构
2.1 系统简介
笔者把设计的这个搜索引擎命名为frgg，下文中将用frgg指代这个搜索引擎。
不过，笔者在此郑重声明：本文中的frgg与网络名人frjj没有一点关系。
frgg运行在一台校内的机器上(http://frgg.3322.org)，每天给成百上千的用户提供查询服务。
截图
@frgg一下.png

frgg是用c语言[]编写，部分自动管理脚本是用Python[]编写，能运行在包括GNU/Linux, FreeBSD等类Unix操作系统之上。

2.2 系统设计理念和原则
   frgg的设计理念和原则
1. 保持简单(Keep It Simple，Stupid)
2. 分而治之(Divide and Conquer)
有人把搜索引擎和操作系统并列成为当今最为复杂的系统软件。的确，在搜索引擎的每一个环节都有很广阔的研究空间，可以把它做得很复杂。
本人采取的策略在不影响可用性的前提下，尽可能保持简单，遇到多种解决方案时，选取简单可行的。
把每个讨论区的文章作为一个独立的处理集合，把版面文章跟精华区文章分开处理，这都是基于BBS按讨论区划分，精华区较庞大，更新较慢，而版面文章较少，更新快速等特点来考虑，

2.2 系统工作流程
整个系统的工作流程如下

首先是后端离线完成的构造索引部分，为用户搜索做好准备：

@buildindex.jpg

下载更新 字符流 字符流 词语 倒排文件
空集-------->文档集合------> ANSI转义字符过滤 ------> 分词 -----> 构造索引 ---->> 索引压缩 --> 通过BDB写入磁盘

接下来就是在线搜索的部分：

查询字符串  查询词list 文档号
用户输入查询 ----------> 过滤分词 ----------> 读入索引-->排序 -----> 生成摘要 ----> 生成结果页面

下一章开始，我们将详细介绍每一个环节.

3.下载系统
在搜索引擎的4大系统中，第1个系统是下载系统.
由于frgg是一个站内搜索，所以下载这一环节是不需要的。
不过为了能使frgg能在BBS服务器以外的机器上提供服务，本文还是实现了简单的抓取文章功能。

frgg下载文章采用增量抓取的策略：
1.在BBS服务器定时生成每个版面的文章列表，frgg定时把文章列表下载下来.
2.用diff工具对比一下新下载的文章列表和当前的文章列表，得出新增的文章和被删除的文章。
3.把新增的文章抓取下来，把已删除的文章从本地删除。

通过这种方式，除了第一次抓取以外，后续的抓取可以很快完成。这大大降低了BBS服务器和本机的负载，使宝贵的校园网带宽不会得到浪费。
在实现上，我们是通过定期读取版面索引文件.DIR来产生文件名列表的。
	frgg下载使用了libcurl[]库，libcurl是一个免费，简单易用的客户端URL传输函数库，支持FTP，HTTP等多个协议。

4 分析系统
搜索引擎的4大系统中的第2个系统是分析系统，分析系统主要的工作包括信息抽取，网页消重，中文分词和PageRank计算等。

4.1 抽取文章内容
4.1.1 解析HTML和链接分析
万维网上大都数的静态网页都是以HTML网页形式存在，它把其描述的全部内容都按照HTML[]语法存放在标签之中。用户通过浏览器看到的是相当友好的信息，而实际上抓回来的源文件中的那些HTML标记，如<HTML>和<TABLE>等都不会实际地展示给用户，因此分析系统要学习浏览器理解网页的方式来理解网页，分析HTML标签。

在实际的网页中，正文，广告及其他各种信息都可能出现在HTML中的<table>和<td>等标签
中。分析系统需要分门别类地从网页中抽取出有价值的能够代表网页的属性，例如锚文本，标
题，和正文等。 并将这些属性结合起来组成一个网页对象。

由于网页的半结构化特性，一方面，标题很容易得到，一般都在<title>标签中。然而得到网页的正文却非常复杂，首先，网页
中没有明显的HTML标签标识出正文；其次正文可能分散在多个HTML标签中，如何组合出完整的正文也是一个问题。
一般通过定义一些规则，然后对每个文本块进行打分，得分高的文本块被认为是正文。

PageRank计算：
在万维网中，实际有意义的，有价值的，以及经常被用户检索的网页规模并没有想象中那么大，一个基本的方法是通过为网页排名使得重要性高而且有价值的网页能够被优先检索。
PageRank就是在这样的应用背景下诞生的。
PageRank的基本思想:
(1) 认可度越高的网页越重要，即反向链接(backlink)越多的网页越重要。
(2) 反向链接的源网页质量越高，被这些高质量网页的链接指向的网页越重要。
(3) 链接数越少的网页越重要。

之前已经提到过，BBS上的文章格式是纯文本的(含有少量ANSI转义序列)，因此这里可以省下不少工作.
1. 我们不必进行链接分析，PageRank计算，frgg是一个站内搜索，而且BBS文章之间没用(或很少)相互引用.
2. 如果是在BBS服务器本地性运行frgg，解析HTML也是不需要的。

如果文章是通过http抓取回来的，那么就需要进行解析出文章的正文出来，不过由于BBS web端产生的文章页面格式是固定，
不像在一般的网页，正文，广告及其他各种信息都可能出现在HTML中的<table>和<td>等标签中。
因此解析程序就不用像通用网页解析程序那样需要考虑很多特殊情况，通过观察BBS文章网页HTML源码可知，我们只需要把<BLOCKQUOTE>和</BLOCKQUOTE>之间的HTML标签跳过，把HTML的转义实体反转回来即可，因此解析起来很简单。

4.1.2 ANSI转以字符序列过滤
接下来要介绍的就是frgg分析系统中要做的事情：ANSI转义字符序列过滤和中文分词。
* ANSI转义字符序列过滤
#介绍ANSI转义字符序列
ANSI转义序列[]是用来为终端控制文本格式和其他输出选项的，包括彩色显示，光标移动等。大部分转义序列都是由字符ESC(十进制27，八进制033)开始，后跟[ (左中括号)，
这两个字符组成的序列叫做控制序列引入者(Control Sequence Introducer)，跟着的字符指定了一个阿拉伯数字用来控制光标或者显示功能.
例如控制图形模式的序列: Esc[Value;...;Valuem some text 会调用相应的图形显示函
数，图形模式改变颜色和文本的属性(例如下划线，粗体)等，指定的函数会在遇到下一个
相同的转义序列之前会一直生效.
例如:
Esc[1;31;42m 这是绿色背景红色亮字 Esc[0m
会显示为
@ansi_escape_seq.png

BBS中部分使用了ANSI转义字符序列，这样使BBS看起来丰富多彩一些，网友们还发明了许多用字符
来拼出各种图案的技术，称之为ASCIIArt，即字符艺术。这种字符图通常用在版面的入口，BBS的进站界面，以及网友的签名档中等. 这是BBS中一道靓丽的风景线.然而，这些风
景对我们来说毫无用处，只增加了我们处理的步骤，这些转义序列字符是没必要索引的，它们夹杂在一般的文字中间，而且它们在浏览器上是不能显示出相应效果来的，反而可能会出
现乱码. 我们首先要过滤掉这些转义序列，即是从文件读入内容时，跳过转义序列，把正常的文字读入到内存中。给接下来的中文分词做准备。

4.2 词语划分
全文检索的检索是通过关键词来进行检索，任何文档都可以看作是一些连续的词的集合，因此首先要把文档分成一个个的词语。
对搜索引擎来说，分词有以下几个目的：
1. 减少使用单字位置信息拼合匹配的次数. 提高效率.
2. 利用分词，提高搜索准确度.

4.2.1 针对英文的处理
在英语文章中，要把文章切分成一个个词语，可以简单地按空格切开，并把标点符号去掉就可以了。
然而也有一些要注意的情况，例如时态和词性的变化，连字符号等情况。通常处理的方法包括词干化(stemming)，大小写转换(casefolding)等。
frgg中把所有英文统一转化为小写来处理，包括文章内容和用户输入的查询。
#去掉停用词(stop words)?
对于一些出现很频繁的词，例如:的，对于检索文档来说价值较小，这种词叫做停用词(stop words)，一般维护着一张停用词列表，在索引阶段跳过这些停用词.这样能使索引占用的空间大大减少. 不过，包含停用词能使检索更准确，而且跳过停用词可能改变了用户查询要表达的意思，比如有些歌曲名就是完全是由停用词组成的((To be or not to be，Let It Be，...)。信息检索系统的趋势是从以前的使用一张很大的停用词列表渐渐到很小的表(20-30个)甚至不使用.现代web搜索引擎通常已经不在使用停用词列表了.

4.2.2 中文分词
然而中文不像英文有空格作为词语间的分隔标志，它没有明显的词间分隔. 这要求在自动分析中文文本前，首先将整句
切割成小的词汇单元，即是中文分词。
例如把句子 “中国航天官员应邀到美国与太空总署官员开会。”
分成一串词：
中国 / 航天 / 官员 / 应邀 / 到 / 美国 / 与 / 太空 / 总署 / 官员 / 开会。

当然我们可以一个个汉字切分开来，这样造成的后果是索引会很庞大，搜索相关性也不强。

#基于词典最大匹配
正向最大匹配(Maximum Matching Method)
最容易想到的，也是最简单的分词办法就是查字典。这种方法最早是由北京航天航空大学的梁南元教授提出的。
简单地说，就是依次读入字符，遇到字典里有的词就标识出来，遇到复合词（比如 “上海大学”）就找最长的词匹配，遇到不认识的字串就分割成单字词，于是简单的分词就完成了。
但是我们都知道，贪心算法并不总是能得到最优解，这里也是一样，比如在分割“研究生命起源”的时候就犯错误了：
研究生/ 命/ 起源
这种方法一个明显的不足是当遇到有二义性（有双重理解意思）的分割时就无能为力了。
比如，对短语 “发展中国家” 正确的分割是“发展 / 中 / 国家”，而从左向右查字典的办法会将它分割成“发展 / 中国 / 家”，显然是错了。
另外，并非所有的最长匹配都一定是正确的。比如“ 上海大学城书店”的正确分词应该是 “上海 / 大学城 / 书店，” 而不是 “上海大学 / 城 / 书店”。
我们发现，如果把最大匹配算法的扫描方向由正向改为反向，精确度可以提高不少。不过有时候得到的输入是一个文本流，就没有办法得到末尾了。

* 其他常见分词技术
#N-Dram
上面提到的方法都有不足，一个称为"N-Gram"的方法可以满足由于分错词而带来的损失，
简单的如2-Gram方法，这种切分方法照顾了所有的可能.举个例子，"中山大学逸仙时空"按照2-Gram的切分方法，将得到"中山"，"山大"，"大学"，"学逸"，"逸仙"，"仙时"和
"时空"。"N-Gram"的分词方法虽然有效的避免字典分词的错误而导致索引不完整的情况，但是回顾我们分词的目的，分词是为了在索引阶段尽量减少索引项，而"N-Gram"的方法，特别是"2-Gram"的分词法却会带来大量的关键词索引项，因为例如"学逸"，"仙时"这样的词都做成了索引项，这是极不经济的。

#基于统计语言模型(Statistical Language Models)
由于词典总是滞后于语言的发展，分词还需要具有新词发现的能力，事实上，发现新出现的词汇相当复杂，基本原理就是探索那些经常一同出现的字，总是相互出现的字很可能构成一个词。90年前后，清华大学的郭进博士用统计语言模型成功解决分词二义性问题，将汉语分词的错误率降低了一个数量级。
利用统计语言模型分词的方法，可以用几个数学公式简单概括如下：
我们假定一个句子S可以有几种分词方法，为了简单起见我们假定有以下三种：
A1，A2，A3，...，Ak，
B1，B2，B3，...，Bm
C1，C2，C3，...，Cn

其中，A1，A2，B1，B2，C1，C2 等等都是汉语的词。那么最好的一种分词方法应该保证分完词后这个句子出现的概率最大。也就是说如果 A1，A2，...，Ak 是最好的分法，那么 （P 表示概率）：
P (A1，A2，A3，...，Ak） > P (B1，B2，B3，...，Bm)，并且
P (A1，A2，A3，...，Ak） > P(C1，C2，C3，...，Cn)

怎么计算P(S) = P(w1，w2，w3，...，wn)的概率呢?
利用条件概率的公式，这个序列出现的概率等于每一个词出现的概率相乘，于是上式可展开为：
P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1)

其中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法n实现。因此我们假定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设[]），于是问题就变得很简单了。现在，S 出现的概率就变为：
P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)…
(当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。）

接下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1，wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中出现了多少次，然后用两个数一除就可以了，P(wi|wi-1) = P(wi-1，wi)/ P (wi-1)。
当然，这里面有一个实现的技巧。如果我们穷举所有可能的分词方法并计算出每种可能性下句子的概率，那么计算量是相当大的。因此，我们可以把它看成是一个动态规划（Dynamic Programming)[] (http://baike.baidu.com/view/28146.htm) 的问题，并利用 “维特比”（Viterbi）[] 算法快速地找到最佳分词。

* frgg采取的分词方法
MMSeg 算法是由 Chih-Hao Tsai 提出的一种基于最大匹配的分词算法。算法以最大匹配为基础，通过几条规则的修正，达到了较高的精确度。按照作者的说法，在一
个 1013 的词的测试输入中，词语的正确识别率达到了 98.41% 。下面简单地介绍一下 MMSEG 算法，更详细的介绍可以参考 Chih-Hao Tsai 的文章[]。
MMSEG 算法主要分为两种：simple 和 complex 。simple 算法就是前面提到的最简单的
正向最大匹配算法。为了解决 simple 算法的不足，MMSEG 又提供了另一种选择：complex
算法。该算法使用了 Chen K. J. 和 Liu S. H. 于 1992 年在 Word identification
for Mandarin Chinese sentences 中提出的一种最大匹配算法的变种。

这种算法的基本思想是：找到所有从当前位置开始的三个连续词语的块，总长度最大的块是最优解。例如，分割“眼看就要来了”这个句子，从“眼”字开始，可能构成的三个连续词的块有（注意每一个单字通常都可以独立成词）：
眼 看 就
眼 看 就要
眼看 就 要
眼看 就要 来
眼看 就要 来了

总长度最大的块就是最后一行的三词块，即最优分解。
按上面规则，有时候还是会得到多个结果，例如，假如到了文本的末尾有“国际化”这个词，三个选择：
国际化
国际 化
国 际 化
拥有同样的最大长度。为了在这种情况下选出一个最优分割，使用了一个最大平均长度的规则（Largest average word length）：即选择块里平均词语长度最大的块。
这条规则只有在文本末尾，无法构成三个词的块时才有用，如果所有的块都是三个词的话，他们的平均长度必然是一样的。
虽然如此，这仍然是一条非常有用的规则，因为文本末尾不仅会出现在文件的末尾，一个句子的末尾（由标点符号标识）也会构成文本末尾。

但是光凭这条规则不足以解决所有的歧义，所以 MMSEG 还使用了另外两条规则：
1. 词语长度变化最小的原则（Smallest variance of word lengths）。例如前面举过的“研究生命起源”的例子就可以用这种方法选出“研究 生命 起源”这个最优的分割，因为三个词的长度都是2，长度变化是0。
2. Largest sum of degree of morphemic freedom of one-character words 规则。通过各个单字在平时被使用的频率数据，就可以用于在不同的块中选出频率最高的一个块。例如“主要是因为”的两个分割“主要/ 是/ 因为”和“主要是因为”，由于单字“是”比单字“主”出现的频率要高，因此可以选出“主要/ 是/ 因为”这个分割，通常这也就是最优分割。
通过这几个额外的规则，MMSEG通常比简单的最大长度匹配算法具有更高的精确度。

此外，分词应该有倾向性. 应该尽量划分出较短的词， 因为有时候会出现长词隐藏短词的问题，比如"中山大学"会把"中山"和"大学"覆盖隐藏，这样查询"大学"会找不到含有"中山大学"的那篇文档。一个解决办法是在分词的同时，找到复合词的嵌套结构。在上面的例子中，如果一句话包含“ 中山大学”四个字，那么先把它当成一个四字词，然后再进一步找出细分词 “中山” 和 “大学”。 


四. 索引系统
在搜索引擎的4大系统中，第3个系统称为"索引系统". 该系统就好像搜索引擎的数据大本营，在这里存储并索引了大量的文档.

假设没有为文档建立索引，那么检索过程的伪代码大概如下:
	** (暴力法 Exhaustive computation)
	for each document d in the collection
	    for each term in query
	             compute similarty of documents and query

使用双重循环，缺点是明显的，效率极其低下，因为大部分文档跟查询是无关的.
我们观察到相关的文档中必须要含有至少一个检索词，因此我们可以使用索引来提高查询效率.
假设我们把每个文档看成是词语的集合，则我们得到了一个 "文档-关键词"矩阵。
建立索引也就意味着把"文档-关键词"矩阵转置为"关键词-文档"矩阵。
在50年代中期，在IBM公司工作的Luhn提出了利用词对文档构建索引并利用检索与文档中词的匹配程度进行检索的方法，这种方
法就是目前常用的倒排文档技术的雏形。

倒排文件：
倒排文件是效率最高的索引结构： 每一个关键词(term)对应着一个记录表(postings list)，记录着含有此词语的文档号(DocId).
这跟书目后面的索引很像，只不过书目后面的索引记录的是页码，而不是文档号。

http://upload.wikimedia.org/wikipedia/commons/d/d9/IVDOC.png


基本的倒排文件由两部分组成，首先有一个关键词词典，对于每个关键词t，记录着一个包含它的文档数和有一个指向它对应着的记录表头部的指针。
第2部分是记录表的集合，每个关键词t对应着一个记录表，保存着一个包含那个关键词的文档号d列表，和关键词在对应文档的出现的频率f(d，t)列表。
这个列表用<d，f(d，t)>序列表示.
上面这种是文档级的索引，没有索引关键词在文档中的位置信息。 不过这已经提供了为布尔查询和相关性排序检索所需的信息。

文档号就是文档的唯一表示符，当然可以通过文档路径(或url)来标识，不过这显然太占空间了.
因此我们首先要为文档进行编号，一般可以通过对文档url进行md5计算，把字符串映射到一个64位的整数。
frgg系统中简单地从1开始按处理顺序给文档进行编号。

建立索引：
建立索引的本质就是矩阵转置，但是对于网页搜索引擎来说，“文档-关键词”矩阵非常稀疏，数据量太大，以致于不能在内存中完成索引的构造。
因此设计建立索引的算法也是由硬件限制和数据量所决定的。

为方便叙述，先定义一些符号.
# t 某个关键词
# d 某篇文档
# tf，Term Frequency的缩写，表示某个关键词在某篇文档中出现的频率。
  tf(d，t)表示t在d中出现的频率。
# N，集合中的文档总数
# df，Document Frequency的缩写，表示文档集合中，出现某个关键词的文档个数。
  df(t)表示t的文档频率.
# idf，Inversed Document Frequency的缩写，文档频率的倒数。
一般定义为 idf(t) = log N/df(t)

#建立索引算法
如果数据量不大，也就是索引能在内存中建立完成，可以使用下面的在内存中建立倒排文件索引算法:
算法I
1. 遍历一次文档集合
   计算每个关键词t的文档频率df，得出对应记录表长度的上界U(t)
2. 分配内存Sum(U(t))，对于每一个关键词t，创建一个指针c(t)指向内存相应块的开头位置。
3. 再一次处理文档集合：
   对每个文档d中的每个关键词t，把文档号d和tf(d，t)添加到内存c(t)中的位置，并更新c(t).
4. 遍历创建的索引，对于每个t，把df(t)个<d，tf(d，t)>写入倒排文件，如果必要，可以压缩之。

上面算法的关键是先收集每个关键词的频率信息，在内存中把倒排索引布好局。然后第2躺才把相应的信息填进去，利用了内存的随机访问的能力。
这种方法的优点是不会造成内存的浪费。但这种遍历两躺文档的算法的缺点就是读取和解析文档占据了大部分的时间。

下面介绍两种只需遍历一趟文档集合的索引算法。
算法II. 按块排序索引算法：
基本过程就是按文档处理顺序建立一个<t，d，tf(d，t)>三元组的数组，达到一定大小时，按关键词t对这个数组进行排序，把结果写入临时文件，最后重复这个过程，最后归并产生最后的倒排文件：

伪代码如下：
  n = 0
  while (all documents have not been processed)
  do n = n + 1
     block = parse_next_block()
     invert(block)
     sort(block)
     write_block_to_disk(block，fn)

  merge_blocks(f1，f2，f3，...，fn，fmerged)

算法III. 基于归并的索引算法：
1 output_file = new file()
2 dict = new hash()
3 while (free memory available)
4 do token = next_token()
5 if token not in dict
6 postinglist = addtodict(dict，token)
7 else postinglist = getpostinglist(dict，token)
8 if full(postinglist)
9 postinglist = doublepostinglist(dict，token)
10 addtopostinglist(postinglist，docid(token))
11 sorted_terms = sortterm(dict)
12 writeblock(sorted_terms，dict，output_file)

算法流程：读取和索引文档直到达到一定的数量，内存不足时，按词典顺序把记录表写入磁盘，最后线性扫描多个临时倒排文件，归并成最后的倒排索引文件。

归并的过程：同时打开所有临时倒排文件，对每一个临时倒排文件，维护一个读缓冲区，一个写缓冲区，每一步，选择还没处理过的词典顺序最小的关键词，这一步可以使用优先级队列或类似的数据结构，把这个关键词的所有记录表读进内存，由于文档号是有序的，可以在线性时间归并这些记录，然后把归并后的记录表写入最终的倒排文件，然后根据需要重新读取内容到缓冲区。

算法III跟算法II不同之处在于算法III每次直接把记录添加到对应的记录表里，
这样有两个优点：1，它不需要排序，因此更快速;
2，它不需要对每个记录都存储它对应的关键词，因此更省空间.
因此，它一次能处理的块比上面算法大得多。

需要说明的是，由于是按文档号(docID)由小到大处理文档，因此新的文档总是插入到记录表(postings lists）的后面。

frgg中建立索引采用的是算法III. 使用Berkeley DB来存储索引.
Berkeley DB（BDB）[]是一个开源的高性能嵌入数据库编程库。应用程序通过简单的函数调用而不是给远程服务器发送消息，BDB消除了SQL查询处理的开销，使应用程序以可预测的方式更快地访问数据，Berkeley DB可以保存任意类型的键／值对，而且可以为一个键保存多个数据。Berkeley DB支持数千的并发线程同时操作数据库，支持最大256TB的数据，广泛用于各种操作系统包括大多数Unix类操作系统和Windows操作系统以及实时操作系统。
由于采用了BDB的BTree的访问方法，因此键是有序的，因此在执行算法III写入磁盘时，就不用额外执行在步骤11行的排序操作了。

由于建立索引都是用文档号(docID)来表示文档，而检索时需要知道文档的真实路径，因此还要记录一个文档号到文档路径的映射关系，这也是通过BDB来存储的。

** 索引压缩与解压
这一节中，我们来讨论索引的压缩和解压。使用压缩一个很明显的好处就是减少存储索引的磁盘空间，我们将会看到使用压缩能使索引减少35%的空间。
使用压缩还有两个微妙的好处，
第一就是能提高cache的命中率，因为内存相对磁盘空间来说是更珍贵的资源，通常使用压缩的主要动机是通过提高cache命中率来提升速度，而不是减少磁盘空间。
第二就是在现代的计算机，高效的解压缩算法运行地很快，传输压缩后的数据块然后再解压缩的过程要比传输未压缩的数据要快，
虽然，解压缩占用了一定时间，但我们读取的是压缩后变得小得多的记录表，大大降低了I/O开销。因此，在大多数检索系统，压缩记录表要比不压缩要运行得快。

** 编码方法
在记录表里，通常每个记录里的词频tf较小，直接用定长的数据类型来保存有点浪费，因此我们考虑使用变长的编码方法。

变长字节编码(Variable byte codes):
这是一种字节对齐的编码方式，即将整数转成二进制后，以7位为单位对其分段。每段前面加一位标志位使其成为8位，恰好用一个字节表示。
首位为1表示为最后1段，0表示还有后续段。
解压过程也很简单，从字节序列中读取，遇到首位是0则继续，遇到首位是1则停止，把读取到的那些7位的段拼接起来。
具体算法如下：
@vbencode.png

还有，由前面可知，记录表的docID是从小到大有序的，因此我们可以只保存docID之间的增量，这种编码方式也称为“差分编码”(gap encoding)。
例如加入文档编号序列为1，17，34，69，489，512，3456，转换为差分序列后得到，1，16，17，35，420，23，2944。序列中的数都变得较小，
能充分利用变长字节编码的优势。

另外还有unrary code，Gamma code等编码方法.下面简单介绍下:
unary code:
      对于x，就是用x个1后面跟1个0来表示。 根据信息论中的理论，可以证明，当1/2的数是1，1/4的数是2，1/8的数是3，...，时，unary编码是最优的.
Gamma code:
      对于 x > 0，把x分解成，2^e + d，其中 e = floor(log2 x)，0 <= d < 2^e，
      然后把(e + 1)用unary编码 ，d直接用2进制表示，把两部分拼接起来就构成了Gamma Code。

frgg中索引采用了变长字节编码，存储文档号之间的差值，通过实验，采用变长编码与不压缩相比，索引大小减少了大约35%，
例如Joke讨论区版面文章索引大小由压缩前的8.7M变为压缩后的4.6M。

#索引精华区
为了索引BBS精华区，我们要更深入了解一下BBS精华区。
前面说过，精华区的结构就像文件系统的目录结构，都是树形结构，每一层目录里除了包含一般文件外，还可以再包含目录。
为了索引精华区，首先要遍历整个精华区，跟讨论区类似，精华区每个目录下有个目录索引文件.DIR，里面保存着当前目录文件的头信息，也就是说
	.DIR ::= annheaders ;
	annheaders ::= annheaders annheader | ;
	annheader ::= struct annheader ;

BBS代码中精华区头信息定义如下:
struct annheader { /* announce header */
       char filename[FNAMELEN]; /* 文件名 */
       char owner[IDLEN + 2]; /* 作者 */
       char editor[IDLEN + 2]; /* 整理 */
       char title[TITLELEN]; /* 标题 */
       unsigned int flag; /* 用来标记本文件是目录还是一般文件等 */
       int mtime; /* 修改时间 */
       char reserved[20]; /* 保留字段 */
};

了解了精华区的结构，就可以采用深度优先遍历算法来遍历整个精华区的文件了。
伪代码如下:
ann_traverse(directory)
	read .DIR
	for each entry in .DIR:
	    if entry is a directory:
	           ann_traverse(entry.filename)
		       else:
			process normal file
				docid++

** 索引的维护:
由于文档集合是变化的，除了新增文章外，文章还会被删除，为了使搜索能反应这种变化，要定时更新索引。
更新索引有几种策略：
1.重新构造
这是最简单直接的方法，适用于文档集合比较小的或者进行动态索引太复杂的时候。

2.把新增的文章索引与原有的索引进行合并。
这与建立索引算法III中处理类似。
如果要求新文档能很快速的查询到，可以维护两个索引，一个大的主索引，还有一个较小的辅助索引，辅助索引停留在内存中。
检索的时候从两部分索引中进行，然后把结果合并。
当辅助索引变得较大时，把它跟主索引进行合并。
对于删除文章的处理，可以建立一个bitmap来记录删除的文章，当返回结果时把这些文章过滤掉。

由于版面文章集合较小，而且文章更新快，frgg遵循简单性原则，对版面文章索引采取重新构造的策略，周期性的产生新索引，待完成后把查询转移到新的索引中进行。


5. 查询系统
在搜索引擎4大系统中，第4个系统成为"查询系统"，查询系统直接面对用户，在接收用户查询请求后，通过检索，排序和摘要提取等计算，
将计算结果组织成搜索结果页面返回给用户。

早期的检索模型是一种称为"布尔模型"(Boolean models)的检索模型。是一种采用AND，OR和NOT等逻辑运算符将多个查询词连成一个布尔表达式，继而通过布尔运算进行检索的简单匹配模型。这个模型把文档看成是词语的集合，即是把每个文档看成是它记录着是否含有那些关键词。
布尔模型的优点是表达简单，易实现，检索速度快，布尔模型的计算主要是求交运算。正因为布尔模型的这些特点也造成了它的不足之处：如果有1个查询词没被包含，则检索失败，缺乏灵活性，检索出来的结果很难进行相关性排序。

# 向量空间检索模型
布尔模型的不足主要没有考虑到关键词在查询中的权重问题，这一点不足在向量空间模型(Vector Space Models)中得到部分解决。
这是由Gerard Salton[]提出的，至今该模型还是信息检索领域最为常用的模型之一。
在此模型中，一个文档(Document)被描述成由一系列关键词(Term)组成的向量。模型并没有规定关键词如何定义，但是一般来说，关键词可以是字，词或者短语。在语音文档检索中，还可以是混淆类、音子、音子串等等单元。假设我们用“词”作为Term，那么在词典中的每一个词，都定义向量空间中的一维。如果一篇文档包含这
个词，那么表示这个文档的向量在这个词所定义的维度上应该拥有一个非0值（对绝大多数系统来说，是正值），每个向量的分量可以理解为包含这个词的权重。

当一个查询被提交时，由于这个查询也是由文本构成，所以也可以被向量空间所表示。模型将对查询与文档，计算一个相似度。
需要注意的是，模型也没有对相似度给出确切的定义。它可以使欧氏距离，也可以是两个向量的夹角的余弦。
向量空间模型主要关心的是“效果”，而非“效率”。
假设\overrightarrow{D}表示文档向量，而\overrightarrow{Q}表示查询向量，文档与查询的相关性可以用余弦距离表示如下：
	Sim(D * Q) = D * Q
	---------
	|D| * |Q|

如果我们用w_{t_iQ}和w_{t_iD}表示\overrightarrow{D}和\overrightarrow{Q}中的第i维的值，并且对每个文档矢量进行归一化，即令\overrightarrow{D}\cdot\overrightarrow{D}\equiv 1，那么上式有可以表示为
Sim(D * Q) = [for t in q:sum(w(d，t) * w(q，t))]
-------------------------------------------
W(d) * W(q)
在此，w_{t_i}究竟如何取值是一个重要的问题，其取值一般被称为关键词i在文档D中的权重。

在向量空间模型下，构造关键词权重计算公式有三个基本原则：
1. 对在较多篇文章出现的词语给于较小的权重，因为这个词区分文档的作用较小。
2. 对在一篇文章出现较多次的词语给于较大的权重，这个词对这篇文章贡献较大。
3. 对于出现较多词语的文章给于较小的权重，可以理解为文档越长，那么出现某个关键词的次数可能越高，而每个关键词对这个文档的区分作用也越低，相应的应该对这些关键词予以一定的折扣。

早期的权重往往直接采用tf，但是显然这种权重并没有考虑上述第一条原则，因此在大规模系统中是不适用的。目前，常用的关键词权重计算公式大多基于tf和df进行构建，同时，一些较为复杂的计算公式也考虑了文档长度。
frgg中采用了以下公式：
w(d，t) = 1 + ln(tf(d，t)); if tf(d，t) > 0
	  0; else
w(q，t) = ln(1 + N / df(t)); if df(t) > 0
	  0; else

W(d) = for t in d:
sqrt(sum(sq(w(d，t)));

W(q) = for t in q:
sqrt(sum(sq(w(q，t)));

因为在计算中，W(q)对每一个来说都是常量，不影响最后的排序，所以在计算中可以不考虑它.
而W(d)可以在建立索引的时候预先计算出来，然后存到一个文件里.

计算过程
@http://nlp.stanford.edu/IR-book/html/htmledition/img442.png


frgg对文档排序采用了布尔模型和向量空间模型结合的方法来进行检索，排序的条件是先考虑文档是否匹配全部查询词，然后再根据余弦计算得分排序。
这样能保证包含全部关键词的文档排在只包含部分关键词匹配的文档(虽然得分更高)的前面.

6. 结果展示
上面检索排序产生了一个文档号列表，当然不可能直接把这个文档号返回给用户，我们需要根据对应的文档产生一个对用户友好的搜索结果页面。
这个页面包括文档的链接，文档的标题，文档的摘要，快照链接等。
如果结果有多页，还需要有多页之间的导航链接，顶部还应该提供一个搜索输入框，按钮，方便用户继续查询。

生成摘要：
文档号列表 自动摘要模块 动态生成搜索结果页
        ----------->
* 自动摘要:
在搜索的结果页面，标题下要显示文本的摘要，但是怎样能使用简短的几句话就可以实现动态摘要呢？
可以有下面几种方法:
一. 只记录关键字在一篇文档中第一次出现的位置
产生动态摘要时，根据关键字第一次出现的位置p向前后扩展，扩展至完整的句子或一定的长度为止. 如果同时检索多个关键字，那么各句动态摘要合并起来，可能中间需要用省略号连接。这种方法产生动态摘要的速度最快，存储位置占用的空间最小; 但是可能产生的动态摘要不是最好的。
二. 记录关键字在一篇文档中所有出现的位置
记录所有的位置可以计算出文档的哪一部分与查询最相关，尤其进行多关键字查询时，将每个关键字出现的所有位置进行统计打分，计算出文档的哪一部分最高分，从而产生动态摘要. 这样产生的动态摘要比较准确，但是存储位置占用的空间比较大。

frgg中，把关键词第一次出现的行开始，连续输出3行作为摘要信息，并高亮显示查询词.

* 快照
有时候，结果页面提供的文章链接可能已经失效了，如果用户看到一篇非常感兴趣的文章而又不能打开时，是非常沮丧的一件事。
因此一般搜索引擎都会提供一个快照链接，里面有那篇文章某一时刻的快照。
	

6. 其他杂项：
*安全隐私
在信息检索系统中，安全是一个很重要的考虑因素。比如说，一个低级别的员工不能查询公司的薪水表，而有权限的经理需要有搜索它的能力。用户查询的结果不能包括他们不能访问
的文档，因为这个文档可能包含敏感信息。用户验证通常使用一种叫做访问控制列表(access control lists)的技术，访问控制列表记录着可以访问每篇文档的用户，然后转置这个矩阵，产生一个对于每个用户，它可以访问的文档列表。最终，这个列表跟搜索结果求交集。

在BBS中，版面也是有访问控制的功能，也是通过类似上述的方法实现的，有访问控
制的版面记录着可以访问该版面的用户列表，考虑到一个有访问控制的版面可能允许
较多人访问，为了提高效率，也把这个“版面--用户”矩阵转置成“用户--版面”矩阵，也
就是对每个用户也记录着一张该用户可以访问的版面，用户访问版面时就直接检查这个
用户的版面访问控制列表。
在frgg中，处理方法也是如此，用户不能查询他不能访问的版面的文章.

CGI程序:
frgg查询的程序是以CGI程序的方式运行：
CGI程序的工作流程
⑴用户在浏览器输入查询请求，通过http协议把请求发给web服务器，服务器根据请求设置一些环境变量，然后执行相应的CGI程序
　　⑵CGI程序从环境变量中获取请求的参数，进行查询计算。
　　⑶CGI程序把处理结果发送回给web服务器。
　　⑷服务器再把结果送回到用户。

frgg --- webserver --- browser 
@cgi如上图

webserver是浏览器跟frgg之间的桥梁，

* 日志系统
为了监控程序运行情况，为改进系统提供参考，frgg记录了各种日志，包括
*查询日志
	搜索引擎能返回用户想要的结果很大程度上是利用了查询日志信息，对其进行事后分析处理，将大都数人认可的结果网页排名靠前，或者给出搜索建议。
	因此一个搜索引擎被越多人使用，其搜索效果就越好，这种查询反馈的机制被搜索引擎大量的地使用。
*调试日志
	这在开发过程中起到很重要的作用，能帮助发现程序错误，方便调试。
*错误日志
	记录了系统运行出现的各种异常情况，利于发现程序中的错误。

*开发过程中使用到了一些工具，包括emacs,gtags,gcc,gdb,svn,GNU make,ltrace,strace,valgrind等，这些工具大大方便了系统的开发。

8. 搜索引擎评价
在信息检索的领域中，检索系统评估对于系统的研究、设计与发展一直有其显着的影响力。 早期对检索系统评估最着名的研究是Cleverdon在1950年代末期开始进行的Cranfield计划，它开创了以测试集（Test Collection）配合测量准则（Measures）来
评估系统的模式。所谓测试集，是一在规范化环境中测试系统效能的机制，包括测试文件集（Document Set） 、测试问题（Queries） 、及相关判断（Relevance
Assessment）等三个部分。其研究设计的概念是假设在给定的查询问句与文件集中，某些文件是与查询问句相关的。系统的目的是检索出相关的文件，并拒绝不相关的文件，
因此采用召回率（Recall）及精确率（Precision）作为测量准则。Cranfield研究首开规范化系统评估之先河，其评估模式亦成为后世普遍采用的标竿。
召回率(Recall)定义为检索系统返回的相关文档比例，也即是有多少相关文档被返回。
精确率(Precision)定义为 检索系统返回的结果中与检索需求相关的文档的比例，也即是返回的结果中有多少是相关的。
文本检索会议(Text Retrieval Conference，TREC)是文本检索领域人气最旺、最权威的评测会议，由美国国防部和美国国家技术标准局（NIST）联合主办。
会议负责组织收集并向与会者提供标准的语料库（Corpus）、检索条件和问题集（Query Set）、以及评测办法（Evaluation），与会者则被要求在规定的时间内构造检索系统并提交检索结果（Runs），由会议负责评测各个检索结果的优劣，最终依据评测结果召开大会进行学术交流，发表会议论文，大大推动了信息检索技术的快速发展。

评价一个搜索引擎系统，一般有两方面，一是效果，二是效率。
在查询效率方面，frgg计算了从获取用户查询到返回结果的时间，并把它显示在结果页面，一般是在0.2秒左右，可以说是查询结果被立即返回了，基本满足了用户的需求。
在效果方面，由于每个人对搜索的需求不同，因此不好简单得从返回的结果来看搜索的效果，我们可以分析查询日志，如果用户点击的搜索结果越靠前，没有翻页，说明效果越好，反之，若用户需要不断修改查询词，或者翻下一页，说明搜索效果不理想。从查询日志来看，大部分查询的效果是不错的。从用户在逸仙时空的回帖来看，用户普遍对frgg的评价是cool! 看来大家对查询效果还是感到满意的。

9. 结论
此次选择逸仙时空搜索引擎的设计和实现作为毕业设计的主题，原因是本人曾担任过逸仙时空BBS的系统维护员，对BBS稍微比较了解，同时在使用BBS过程中经常听到用户的疑问或者抱怨，逸仙时空怎么查找文章？同时，搜索引擎技术与本人所学的专业知识联系比较紧密，把本人所学知识应用到实际项目中去，这是十分有意义的。最终，本人设计实现了这个搜索引擎，为逸仙时空提供了一个全文检索的功能，达到了整合信息资源，方便用户获取信息的目的。
本文对搜索引擎的工作流程做了大体的介绍，描述总结了设计实现一个小型搜索引擎过程。通过这个系统的编写，本人系统了解了搜索引擎各个方面的知识，
锻炼了项目开发的能力。此外，这个搜索引擎虽然是针对BBS的，但它具有一定的通用性，可以根据具体应用改造成相应的全文检索系统。
当然，这个系统也还有很多不足之处和需要完善的地方，本人以后也将会继续改造这个系统，在此提几点改进的方向：
1. 设计开发一个telnet端的查询的功能。
2. 可以针对文章的元信息建立索引，BBS文章元信息格式固定(如作者，发表时间)，每篇文章的前三行。
3. 改进中文分词，自动摘要的效果。对于分词，可以考虑给每个版面使用一个额外的词典，比如在电影版，可以使用一个电影名大全的词典。
4. 另外，对搜索结果进行缓存也是搜索引擎很重要的一个部分，特别是当访问量很大的时候。
5. 对快照文件进行压缩存储，节省空间。


致谢:
感谢我的导师张治国老师对我的悉心指导，感谢中山大学逸仙时空BBS提供研究和实践的平台。
感谢我的室友和师兄师姐们提供帮助。
感谢中大netshare提供机器，使得frgg能够稳定地运行。
最后，再次对所有关心、帮助，教导过我的老师和同学表示衷心地感谢。

参考文献
[1] V. Bush，“As We May Think”，Atlantic Monthly，vol. 176，pp. 101–108，1945
[2],The first search engine, Archie, http://www.isrl.illinois.edu/~chip/projects/timeline/1990archie.htm
[3], http://en.wikipedia.org/wiki/World_Wide_Web_Wanderer
[4], David Eichmann, The Repository Based Software Engineering Program, Fifth Systems Reengineering
Technology Workshop, Monterey, CA, Feb. 7-9, 1995
[5] 文本信息检索，http://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2
[6] 有关BBS的问答
[7] kxn，重负载Telnet BBS系统优化和维护经验谈
[8] kyhpudding，乱弹 telnet based BBS
[9] W3 SEARCH ENGINES，http://vlib.iue.it/history/search/
[10] 梁斌，走进搜索引擎，电子工业出版社，2007
[11] Justin Zobel，Inverted Files for Text Search Engines，2006
[12] Christopher D. Manning，Prabhakar Raghavan and Hinrich Schütze，Introduction to Information Retrieval，Cambridge University Press. 2008.
[13] Brian W. Kernighan，Dennis M. Ritchie，The C Programming Language，Prentice Hall，Inc.，1988.
[14] Python, http://www.python.org
[15] libcurl - the multiprotocol file transfer library，http://curl.haxx.se/libcurl/
[16] w3c，HTML 4.01 Specification，http://www.w3.org/TR/html401/
[14] ANSI escape code，http://en.wikipedia.org/wiki/ANSI_escape_code
[15] ANSI Escape sequences，http://ascii-table.com/ansi-escape-sequences.php
[16]，吴军，谈谈中文分词 ----- 统计语言模型在中文处理中的一个应用，http://www.googlechinablog.com/2006/04/blog-post_10.html
[17]，吴军，统计语言模型，http://googlechinablog.com/2006/04/blog-post.html
[18]，Markov Chain, http://en.wikipedia.org/wiki/Markov_chain
[18]，Viterbi Algorithm，http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s2_pg1.html
[19] Chih-Hao Tsai，MMSEG: A Word Identification System for Mandarin Chinese Text
Based on Two Variants of the Maximum Matching Algorithm，
http://technology.chtsai.org/mmseg/
[20] Chen，K. J.，& Liu，S. H. (1992). Word identification for Mandarin Chinese
sentences. Proceedings of the Fifteenth International Conference on
Computational Linguistics，Nantes: COLING-92.
[21] pluskid，RMMSeg: Ruby 实现中文分词，http://lifegoo.pluskid.org/?p=261
[22] 黄嘉欣，校园网搜索引擎系统的设计与实现，2008
[23] Backus–Naur Form，http://en.wikipedia.org/wiki/Backus%E2%80%93Naur_Form

* Version Control with Subversion，http://svnbook.red-bean.com/en/1.5/index.html
* Getting Started with Berkeley DB，http://www.oracle.com/technology/documentation/berkeley-db/db/gsg/C/index.html
* The Common Gateway Interface (CGI) Version 1.1，http://tools.ietf.org/html/rfc3875
* GNU `make'，http://www.gnu.org/software/make/manual/make.html

* 杨建武，文档自动摘要技术 http://www.icst.pku.edu.cn/course/TextMining/07-08Spring/TextMining09-摘要.pdf
* 如何生成搜索结果中的动态摘要，http://www.liujie.org.cn/archives/55
[] 百度，http://www.baidu.com
[] Google，http://www.google.com
[] 有道，http://www.youdao.com

[] 闫宏飞，测试集（Test Collection），2004

附录
各部分代码功能解释：

ANSI转义序列过滤 src/src/ansifileter.c
分词的实现代码见 src/src/segment.c
建造索引 src/src/index.c
检索排序：src/src/search.c
CGI应用程序： src/src/cgi.c
头文件目录   src/include/	
	        		

术语表英文对照：
BBS
关键词 term
倒排文件 inverted file
向量空间模型 vector space model
爬虫 spider，crawler
精华区
记录表 postings list
BDB
